{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bahador1/BahadorColabNotes/blob/main/dlg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import pickle\n",
        "import PIL.Image as Image"
      ],
      "metadata": {
        "id": "_ZWJKRp8zp65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self, channel=3, hideen=768, num_classes=10):\n",
        "        super(LeNet, self).__init__()\n",
        "        act = nn.Sigmoid\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Conv2d(channel, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
        "            act(),\n",
        "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
        "            act(),\n",
        "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=1),\n",
        "            act(),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hideen, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.body(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    try:\n",
        "        if hasattr(m, \"weight\"):\n",
        "            m.weight.data.uniform_(-0.5, 0.5)\n",
        "    except Exception:\n",
        "        print('warning: failed in weights_init for %s.weight' % m._get_name())\n",
        "    try:\n",
        "        if hasattr(m, \"bias\"):\n",
        "            m.bias.data.uniform_(-0.5, 0.5)\n",
        "    except Exception:\n",
        "        print('warning: failed in weights_init for %s.bias' % m._get_name())\n"
      ],
      "metadata": {
        "id": "FTtqXL0NABmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_from_Image(Dataset):\n",
        "    def __init__(self, imgs, labs, transform=None):\n",
        "        self.imgs = imgs # img paths\n",
        "        self.labs = labs # labs is ndarray\n",
        "        self.transform = transform\n",
        "        del imgs, labs\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labs.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        lab = self.labs[idx]\n",
        "        img = Image.open(self.imgs[idx])\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        return img, lab\n",
        "\n"
      ],
      "metadata": {
        "id": "A-VDvGIoAC23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def main():\n",
        "dataset = 'MNIST'\n",
        "root_path = '.'\n",
        "data_path = os.path.join(root_path, '../data').replace('\\\\', '/')\n",
        "save_path = os.path.join(root_path, 'results/iDLG_%s'%dataset).replace('\\\\', '/')\n",
        "\n",
        "# lr = 1.0\n",
        "num_dummy = 1\n",
        "Iteration = 300\n",
        "# num_exp = 1000\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = 'cuda' if use_cuda else 'cpu'\n",
        "\n",
        "tt = transforms.Compose([transforms.ToTensor()])\n",
        "tp = transforms.Compose([transforms.ToPILImage()])\n",
        "\n",
        "print(dataset, 'root_path:', root_path)\n",
        "print(dataset, 'data_path:', data_path)\n",
        "print(dataset, 'save_path:', save_path)\n",
        "\n",
        "if not os.path.exists('results'):\n",
        "    os.mkdir('results')\n",
        "if not os.path.exists(save_path):\n",
        "    os.mkdir(save_path)\n",
        "\n",
        "\n",
        "\n",
        "''' load data '''\n",
        "if dataset == 'MNIST':\n",
        "    shape_img = (28, 28)\n",
        "    num_classes = 10\n",
        "    channel = 1\n",
        "    hidden = 588\n",
        "    dst = datasets.MNIST(data_path, download=True)"
      ],
      "metadata": {
        "id": "aZ-HJuAlAEAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8dd26e8-6f0c-4d62-bf5b-430b4172f07a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST root_path: .\n",
            "MNIST data_path: ./../data\n",
            "MNIST save_path: ./results/iDLG_MNIST\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 58.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.71MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.8MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.08MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' train DLG and iDLG '''\n",
        "# for idx_net in range(num_exp):\n",
        "net = LeNet(channel=channel, hideen=hidden, num_classes=num_classes)\n",
        "net.apply(weights_init)\n",
        "\n",
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "vGX34hs5X5eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_shuffle = np.random.permutation(len(dst))\n",
        "\n",
        "# for method in ['DLG', 'iDLG']:\n",
        "method = 'DLG'\n",
        "print('%s, Try to generate %d images' % (method, num_dummy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxvfOkQpfqJF",
        "outputId": "1a1b2e5d-7a62-4735-b7eb-a2af2b32279c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DLG, Try to generate 1 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imidx_list = []\n",
        "for imidx in range(num_dummy):\n",
        "    idx = idx_shuffle[imidx]\n",
        "    imidx_list.append(idx)\n",
        "    tmp_datum = tt(dst[idx][0]).float().to(device)\n",
        "    tmp_datum = tmp_datum.view(1, *tmp_datum.size())\n",
        "    tmp_label = torch.Tensor([dst[idx][1]]).long().to(device)\n",
        "    tmp_label = tmp_label.view(1, )\n",
        "    if imidx == 0:\n",
        "        gt_data = tmp_datum\n",
        "        gt_label = tmp_label\n",
        "    # else:\n",
        "    #     gt_data = torch.cat((gt_data, tmp_datum), dim=0)\n",
        "    #     gt_label = torch.cat((gt_label, tmp_label), dim=0)"
      ],
      "metadata": {
        "id": "SW_GZIPLfu75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# compute original gradient\n",
        "out = net(gt_data)\n",
        "loss = criterion(out, gt_label)\n",
        "dloss_dx = torch.autograd.grad(loss , net.parameters())\n",
        "original_dy_dx = list((_.detach().clone() for _ in dloss_dx))\n",
        "\n",
        "# generate dummy data and label\n",
        "dummy_data = torch.randn(gt_data.size()).to(device).requires_grad_(True)\n",
        "dummy_label = torch.randn((gt_data.shape[0], num_classes)).to(device).requires_grad_(True)\n",
        "\n",
        "#this optimizer is for updating dummy data.\n",
        "optimizer = torch.optim.Adam([dummy_data, dummy_label], lr=1.0)\n",
        "\n",
        "#point 1\n",
        "history = []\n",
        "history_iters = []\n",
        "losses = []\n",
        "mses = []\n",
        "train_iters = []\n",
        "\n",
        "for iters in range(Iteration):\n",
        "\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        pred = net(dummy_data)\n",
        "        # if method == 'DLG':\n",
        "        dummy_loss = - torch.mean(torch.sum(torch.softmax(dummy_label, -1) * torch.log(torch.softmax(pred, -1)), dim=-1))\n",
        "        # dummy_loss = criterion(pred, gt_label)\n",
        "\n",
        "        dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
        "\n",
        "        grad_diff = 0\n",
        "        for gx, gy in zip(dummy_dy_dx, original_dy_dx):\n",
        "            grad_diff += ((gx - gy) ** 2).sum()\n",
        "\n",
        "        grad_diff.backward()\n",
        "        return grad_diff\n",
        "\n",
        "    optimizer.step(closure) # b sabke LBFGS\n",
        "    current_loss = closure().item()\n",
        "    train_iters.append(iters)\n",
        "    losses.append(current_loss)\n",
        "    mses.append(torch.mean((dummy_data-gt_data)**2).item())\n",
        "\n",
        "\n",
        "    if iters % int(Iteration / 30) == 0:\n",
        "        current_time = str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n",
        "        print(current_time, iters, 'D_i = %.8f, rec error = %.8f' %(current_loss, mses[-1]))\n",
        "        history.append([tp(dummy_data[imidx].cpu()) for imidx in range(num_dummy)])\n",
        "        history_iters.append(iters)\n",
        "\n",
        "        for imidx in range(num_dummy):\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            plt.subplot(3, 10, 1)\n",
        "            plt.imshow(tp(gt_data[imidx].cpu()))\n",
        "            for i in range(min(len(history), 29)):\n",
        "                plt.subplot(3, 10, i + 2)\n",
        "                plt.imshow(history[i][imidx])\n",
        "                plt.title('iter=%d' % (history_iters[i]))\n",
        "                plt.axis('off')\n",
        "            if method == 'DLG':\n",
        "                plt.savefig('%s/DLG_on_%s_%05d.png' % (save_path, imidx_list, imidx_list[imidx]))\n",
        "                plt.close()\n",
        "\n",
        "\n",
        "        if current_loss < 0.000001: # converge\n",
        "            print(\"it has converged\")\n",
        "            break\n",
        "\n",
        "if method == 'DLG':\n",
        "    loss_DLG = losses\n",
        "    label_DLG = torch.argmax(dummy_label, dim=-1).detach().item()\n",
        "    mse_DLG = mses\n",
        "\n",
        "\n",
        "print('imidx_list:', imidx_list)\n",
        "print('loss_DLG:', loss_DLG[-1],)\n",
        "print('mse_DLG:', mse_DLG[-1], )\n",
        "print('gt_label:', gt_label.detach().cpu().data.numpy(), 'lab_DLG:', label_DLG, )\n",
        "\n",
        "print('----------------------\\n\\n')"
      ],
      "metadata": {
        "id": "pVVBCTiSyFsG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49eaaa46-b51d-4977-c773-7a2f69476283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-06 15:13:34] 0 D_i = 46.45686340, rec error = 2.02256799\n",
            "[2025-04-06 15:13:34] 10 D_i = 18.11983490, rec error = 17.65144157\n",
            "[2025-04-06 15:13:36] 20 D_i = 12.07498360, rec error = 26.97108650\n",
            "[2025-04-06 15:13:36] 30 D_i = 10.43585396, rec error = 31.13733673\n",
            "[2025-04-06 15:13:36] 40 D_i = 9.93463135, rec error = 32.86661530\n",
            "[2025-04-06 15:13:37] 50 D_i = 9.51446533, rec error = 33.58797455\n",
            "[2025-04-06 15:13:37] 60 D_i = 9.20694733, rec error = 33.92465210\n",
            "[2025-04-06 15:13:37] 70 D_i = 8.90260029, rec error = 34.12365341\n",
            "[2025-04-06 15:13:38] 80 D_i = 8.69240761, rec error = 34.26870346\n",
            "[2025-04-06 15:13:38] 90 D_i = 8.52081490, rec error = 34.38152695\n",
            "[2025-04-06 15:13:39] 100 D_i = 8.37277222, rec error = 34.46982956\n",
            "[2025-04-06 15:13:39] 110 D_i = 8.23149872, rec error = 34.54435730\n",
            "[2025-04-06 15:13:40] 120 D_i = 8.09077168, rec error = 34.61719131\n",
            "[2025-04-06 15:13:40] 130 D_i = 7.95240927, rec error = 34.69346237\n",
            "[2025-04-06 15:13:41] 140 D_i = 7.82345533, rec error = 34.76489258\n",
            "[2025-04-06 15:13:42] 150 D_i = 7.69927120, rec error = 34.83030319\n",
            "[2025-04-06 15:13:42] 160 D_i = 7.58818531, rec error = 34.89558792\n",
            "[2025-04-06 15:13:43] 170 D_i = 7.49179459, rec error = 34.96377563\n",
            "[2025-04-06 15:13:44] 180 D_i = 7.40680408, rec error = 35.03527832\n",
            "[2025-04-06 15:13:45] 190 D_i = 7.33019781, rec error = 35.10755539\n",
            "[2025-04-06 15:13:46] 200 D_i = 7.26757479, rec error = 35.17758179\n",
            "[2025-04-06 15:13:46] 210 D_i = 7.21612692, rec error = 35.24179459\n",
            "[2025-04-06 15:13:47] 220 D_i = 7.16995430, rec error = 35.29952240\n",
            "[2025-04-06 15:13:48] 230 D_i = 7.12700224, rec error = 35.35332108\n",
            "[2025-04-06 15:13:49] 240 D_i = 7.08639717, rec error = 35.40559006\n",
            "[2025-04-06 15:13:50] 250 D_i = 7.04777718, rec error = 35.45758057\n",
            "[2025-04-06 15:13:51] 260 D_i = 7.01120710, rec error = 35.51022339\n",
            "[2025-04-06 15:13:52] 270 D_i = 6.97652960, rec error = 35.56446838\n",
            "[2025-04-06 15:13:53] 280 D_i = 6.94321394, rec error = 35.62081146\n",
            "[2025-04-06 15:13:54] 290 D_i = 6.91077471, rec error = 35.67922592\n",
            "imidx_list: [np.int64(44075)]\n",
            "loss_DLG: 6.882046699523926\n",
            "mse_DLG: 35.73341751098633\n",
            "gt_label: [9] lab_DLG: 1\n",
            "----------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NZTZ288UwZ15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `create_graph` in `torch.autograd.grad()`"
      ],
      "metadata": {
        "id": "UrNWJJsIqNEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "xnY3K7EBqaYc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2., requires_grad = True)\n",
        "y = x**2"
      ],
      "metadata": {
        "id": "jN_BThvbqb33"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad = torch.autograd.grad(y, x )# dy/dx"
      ],
      "metadata": {
        "id": "xq5WTmbAqgOE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.autograd.grad(grad,x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "YPF4N7owqqbj",
        "outputId": "7982954d-03d6-4890-c975-d3d904ccd902"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c5fe3babd7be>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n\u001b[1;32m    495\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         result = _engine_run_backward(\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fG-dHU-Br2_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### here is the example shows why it is hard to calculate the gradients of intermediate tensors using `.backward()` compare to `torch.autograd.grad()`.\n"
      ],
      "metadata": {
        "id": "ShWLhrTcyX1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "AJgbaquyyvvJ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2., requires_grad = True)\n",
        "y = x**2\n",
        "z = y**3"
      ],
      "metadata": {
        "id": "oKhhj2yNzQdG"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward()"
      ],
      "metadata": {
        "id": "Vg-tGFkbzkc4"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcsDNXCNz856",
        "outputId": "09aa317b-6f67-4c7f-ce86-6670ec9d9e72"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-10b3a7061f6d>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  y.grad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hv89s8Yd6IVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `retain_graph` is also useless for calculating intermediate tensors' grads"
      ],
      "metadata": {
        "id": "n4p8bfFe62t_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "3LvjNGBE7HZu"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2., requires_grad = True)\n",
        "y = x**2\n",
        "z = y**3"
      ],
      "metadata": {
        "id": "iJu7hNfw7NCl"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward(retain_graph =True)"
      ],
      "metadata": {
        "id": "ZjI9deT-7OIO"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVkuubSQ7QCP",
        "outputId": "299af30b-5cc2-450a-d75e-713627023126"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-69-10b3a7061f6d>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  y.grad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward(retain_graph = True)"
      ],
      "metadata": {
        "id": "3D29LiDR7Q2_"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVluwcS47TKu",
        "outputId": "cddacef7-c140-4411-cdfd-39f7fe3328b6"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-10b3a7061f6d>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  y.grad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x ** 2\n",
        "z = y ** 3\n",
        "\n",
        "# Get ∂z/∂y directly\n",
        "dz_dx  = torch.autograd.grad(z, x, create_graph = True)"
      ],
      "metadata": {
        "id": "iPWEPhgM7zD3"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dz_dy = torch.autograd.grad(z, y)"
      ],
      "metadata": {
        "id": "6dnzG1Y1HTTW"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dz_dy"
      ],
      "metadata": {
        "id": "et0qIVpgHY5o",
        "outputId": "9199bd2a-6d74-429c-e6ad-a78792d4178b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(48.),)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MwzT8CSRH5Iw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}