{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bahador1/BahadorColabNotes/blob/main/dlg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import pickle\n",
        "import PIL.Image as Image"
      ],
      "metadata": {
        "id": "_ZWJKRp8zp65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self, channel=3, hideen=768, num_classes=10):\n",
        "        super(LeNet, self).__init__()\n",
        "        act = nn.Sigmoid\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Conv2d(channel, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
        "            act(),\n",
        "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
        "            act(),\n",
        "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=1),\n",
        "            act(),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hideen, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.body(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    try:\n",
        "        if hasattr(m, \"weight\"):\n",
        "            m.weight.data.uniform_(-0.5, 0.5)\n",
        "    except Exception:\n",
        "        print('warning: failed in weights_init for %s.weight' % m._get_name())\n",
        "    try:\n",
        "        if hasattr(m, \"bias\"):\n",
        "            m.bias.data.uniform_(-0.5, 0.5)\n",
        "    except Exception:\n",
        "        print('warning: failed in weights_init for %s.bias' % m._get_name())\n"
      ],
      "metadata": {
        "id": "FTtqXL0NABmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_from_Image(Dataset):\n",
        "    def __init__(self, imgs, labs, transform=None):\n",
        "        self.imgs = imgs # img paths\n",
        "        self.labs = labs # labs is ndarray\n",
        "        self.transform = transform\n",
        "        del imgs, labs\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labs.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        lab = self.labs[idx]\n",
        "        img = Image.open(self.imgs[idx])\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        return img, lab\n",
        "\n"
      ],
      "metadata": {
        "id": "A-VDvGIoAC23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def main():\n",
        "dataset = 'MNIST'\n",
        "root_path = '.'\n",
        "data_path = os.path.join(root_path, '../data').replace('\\\\', '/')\n",
        "save_path = os.path.join(root_path, 'results/iDLG_%s'%dataset).replace('\\\\', '/')\n",
        "\n",
        "# lr = 1.0\n",
        "num_dummy = 1\n",
        "# num_exp = 1000\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = 'cuda' if use_cuda else 'cpu'\n",
        "\n",
        "tt = transforms.Compose([transforms.ToTensor()])\n",
        "tp = transforms.Compose([transforms.ToPILImage()])\n",
        "\n",
        "print(dataset, 'root_path:', root_path)\n",
        "print(dataset, 'data_path:', data_path)\n",
        "print(dataset, 'save_path:', save_path)\n",
        "\n",
        "if not os.path.exists('results'):\n",
        "    os.mkdir('results')\n",
        "if not os.path.exists(save_path):\n",
        "    os.mkdir(save_path)\n",
        "\n",
        "\n",
        "\n",
        "''' load data '''\n",
        "if dataset == 'MNIST':\n",
        "    shape_img = (28, 28)\n",
        "    num_classes = 10\n",
        "    channel = 1\n",
        "    hidden = 588\n",
        "    dst = datasets.MNIST(data_path, download=True)"
      ],
      "metadata": {
        "id": "aZ-HJuAlAEAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c323898-09cb-4fb7-e6fc-57e01533a2f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST root_path: .\n",
            "MNIST data_path: ./../data\n",
            "MNIST save_path: ./results/iDLG_MNIST\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 6.07MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 160kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.30MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.60MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' train DLG and iDLG '''\n",
        "# for idx_net in range(num_exp):\n",
        "net = LeNet(channel=channel, hideen=hidden, num_classes=num_classes)\n",
        "net.apply(weights_init)\n",
        "\n",
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "vGX34hs5X5eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_shuffle = np.random.permutation(len(dst))\n",
        "\n",
        "# for method in ['DLG', 'iDLG']:\n",
        "method = 'DLG'\n",
        "print('%s, Try to generate %d images' % (method, num_dummy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxvfOkQpfqJF",
        "outputId": "9b5db997-444a-4748-fb16-a400de5539e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DLG, Try to generate 1 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imidx_list = []\n",
        "for imidx in range(num_dummy):\n",
        "    idx = idx_shuffle[imidx]\n",
        "    imidx_list.append(idx)\n",
        "    tmp_datum = tt(dst[idx][0]).float().to(device)\n",
        "    tmp_datum = tmp_datum.view(1, *tmp_datum.size())\n",
        "    tmp_label = torch.Tensor([dst[idx][1]]).long().to(device)\n",
        "    tmp_label = tmp_label.view(1, )\n",
        "    if imidx == 0:\n",
        "        gt_data = tmp_datum\n",
        "        gt_label = tmp_label\n",
        "    # else:\n",
        "    #     gt_data = torch.cat((gt_data, tmp_datum), dim=0)\n",
        "    #     gt_label = torch.cat((gt_label, tmp_label), dim=0)"
      ],
      "metadata": {
        "id": "SW_GZIPLfu75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gt_data.requires_grad"
      ],
      "metadata": {
        "id": "1CTZ_m3958Or",
        "outputId": "da9cd1a2-5566-4d3d-d5c6-6706cd14b5dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "#original Grad: forward pass\n",
        "out = net(gt_data)\n",
        "loss = criterion(out, gt_label)\n",
        "\n",
        "#original Grad: backward phase\n",
        "dloss_dx = torch.autograd.grad(loss , net.parameters())       # dloss(x, w)/ dw_i\n",
        "original_dy_dx = list((_.detach().clone() for _ in dloss_dx)) # original gradient Tensors\n",
        "\n",
        "# generate dummy data and label\n",
        "dummy_data  = torch.randn(gt_data.size()).to(device).requires_grad_(True)\n",
        "dummy_label = torch.randn((gt_data.shape[0], num_classes)).to(device).requires_grad_(True)\n",
        "\n",
        "#this optimizer is for updating dummy data.\n",
        "optimizer = torch.optim.LBFGS([dummy_data, dummy_label], lr=1.0, )\n",
        "\n",
        "history = []\n",
        "history_iters = []\n",
        "losses = []\n",
        "mses = []\n",
        "train_iters = []\n",
        "\n",
        "Iteration = 300\n",
        "\n",
        "\n",
        "for iters in range(Iteration):\n",
        "\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        pred = net(dummy_data)\n",
        "        print(\"pred\", pred)\n",
        "        # if method == 'DLG':\n",
        "        # dummy_loss = - torch.mean(torch.sum(torch.softmax(dummy_label, -1) * torch.log(torch.softmax(pred, -1)), dim=-1))\n",
        "        dummy_loss = criterion(pred, gt_label)\n",
        "\n",
        "        dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
        "        # print(\"dummy_dy_dw\", torch.mean(dummy_dy_dx[0]))\n",
        "        return\n",
        "        grad_diff = 0\n",
        "        for gx, gy in zip(dummy_dy_dx, original_dy_dx):\n",
        "            grad_diff += ((gx - gy) ** 2).sum()\n",
        "\n",
        "        grad_diff.backward()\n",
        "        return grad_diff\n",
        "\n",
        "    optimizer.step(closure) # b sabke LBFGS\n",
        "    current_loss = closure().item()\n",
        "    train_iters.append(iters)\n",
        "    losses.append(current_loss)\n",
        "    mses.append(torch.mean((dummy_data-gt_data)**2).item())\n",
        "\n",
        "\n",
        "    if iters % int(Iteration / 30) == 0:\n",
        "        current_time = str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n",
        "        print(current_time, iters, 'D_i = %.8f, rec error = %.8f' %(current_loss, mses[-1]))\n",
        "        history.append([tp(dummy_data[imidx].cpu()) for imidx in range(num_dummy)])\n",
        "        history_iters.append(iters)\n",
        "\n",
        "        for imidx in range(num_dummy):\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            plt.subplot(3, 10, 1)\n",
        "            plt.imshow(tp(gt_data[imidx].cpu()))\n",
        "            for i in range(min(len(history), 29)):\n",
        "                plt.subplot(3, 10, i + 2)\n",
        "                plt.imshow(history[i][imidx])\n",
        "                plt.title('iter=%d' % (history_iters[i]))\n",
        "                plt.axis('off')\n",
        "            if method == 'DLG':\n",
        "                plt.savefig('%s/DLG_on_%s_%05dMINE.png' % (save_path, imidx_list, imidx_list[imidx]))\n",
        "                plt.close()\n",
        "\n",
        "\n",
        "        if current_loss < 0.000001: # converge\n",
        "            print(\"it has converged\")\n",
        "            break\n",
        "\n",
        "if method == 'DLG':\n",
        "    loss_DLG = losses\n",
        "    label_DLG = torch.argmax(dummy_label, dim=-1).detach().item()\n",
        "    mse_DLG = mses\n",
        "\n",
        "\n",
        "print('imidx_list:', imidx_list)\n",
        "print('loss_DLG D_i:', loss_DLG[-1],)\n",
        "print('mse final x` and x :', mse_DLG[-1], )\n",
        "# print('gt_label:', gt_label.detach().cpu().data.numpy(), 'lab_DLG:', label_DLG, )\n",
        "\n",
        "print('----------------------\\n\\n')"
      ],
      "metadata": {
        "id": "pVVBCTiSyFsG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "0315a16a-2de9-4dd9-9420-130b743aa39b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred tensor([[ 4.6850, -0.0552, 11.8238, -1.8587,  8.4669, -4.3711, -7.1058,  3.9937,\n",
            "          1.6205, -5.1649]], grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "float() argument must be a string or a real number, not 'NoneType'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-90d3d736efa4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_diff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# b sabke LBFGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mtrain_iters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# evaluate initial f(x) and df/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0morig_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0mcurrent_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"func_evals\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'NoneType'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x95-eE7VAl_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outputs"
      ],
      "metadata": {
        "id": "eflIF4-zjVTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Trainable Parameters: 28219\n",
        "\n",
        "Training begins...\n",
        "EPOCH:0 STEP:0\n",
        "Total Loss:4.137        Classification Loss:2.915       Robustness Loss:0.730   Concept Loss:1.222      Accuracy:0.000\n",
        "EPOCH:0 STEP:100\n",
        "Total Loss:2.522        Classification Loss:1.439       Robustness Loss:0.384   Concept Loss:1.083      Accuracy:1.000\n",
        "EPOCH:0 STEP:200\n",
        "Total Loss:2.330        Classification Loss:1.850       Robustness Loss:0.580   Concept Loss:0.480      Accuracy:1.000\n",
        "^CCTRL+C pressed... Waiting to finalize.\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "wHMpHzFgjItg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create the\n",
        "1. understand `F.nll_loss`\n",
        "2. does `log(softmax(pred))` equal to `F.log_softmax()`?"
      ],
      "metadata": {
        "id": "wxVpUQ5Nb64b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.tensor([\n",
        "    [2.0, 1.0, 0.1, -1.0],\n",
        "    [1.2, -0.5, 0.3, 0.0],\n",
        "    [0.0, 0.5, 2.5, -1.0]\n",
        "], requires_grad=True)\n",
        "\n",
        "log_probs = F.log_softmax(logits, dim=1)\n",
        "\n",
        "target = torch.tensor([0, 2, 1])  # class indices\n",
        "\n",
        "loss = F.nll_loss(log_probs, target)"
      ],
      "metadata": {
        "id": "8sLUMD0tjSO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.log(torch.softmax(pred, -1)) vs F.log_softmax()"
      ],
      "metadata": {
        "id": "Ebo1t5tKYQbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Dummy dataset\n",
        "X = torch.randn(100, 10)        # 100 samples, 10 features\n",
        "y = torch.randint(0, 3, (100,)) # 100 labels (3 classes: 0, 1, or 2)\n",
        "\n",
        "# Simple model: fully connected layer + log_softmax\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 50)\n",
        "        self.fc2 = nn.Linear(50, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)  # NLLLoss expects log-probabilities\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(X)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = F.nll_loss(output, y)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "ZpCOEQ8bf-mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OFFNa--QDZC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "### dummy data\n",
        "x = torch.randn(100, 3)\n",
        "y = torch.randint(0, 3, (100,))\n",
        "\n"
      ],
      "metadata": {
        "id": "Tw6iomMlDa1a"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "ZoQ4RHuvFzPm",
        "outputId": "23fe1f61-2f9c-4cba-a560-aebad72de08b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1  = nn.Linear(3, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "UjNAZTczEL72"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()"
      ],
      "metadata": {
        "id": "vIWXL7nhERjU"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1.0)"
      ],
      "metadata": {
        "id": "ISzByxjAEvSu"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for Epoch in range(20):\n",
        "    pred = model(x)\n",
        "    loss  = F.nll_loss(pred, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "iQ9PtZ6OE9Bj",
        "outputId": "467ec02d-7762-49bd-80d8-1b8ae76f92c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n",
            "size of x  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(pred[0])"
      ],
      "metadata": {
        "id": "BaU-AILoFgsk",
        "outputId": "eda22f37-d766-4238-9c60-0e9c223b0c3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-3.3255, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_x = torch.tensor([[1., 2., 3.]])"
      ],
      "metadata": {
        "id": "edMR9URrHSdv"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F.softmax(dummy_x, dim = 0)"
      ],
      "metadata": {
        "id": "P1emuZGwHYrT",
        "outputId": "dd285a74-f7ac-4155-b4eb-3552f599186f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0900, 0.2447, 0.6652])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.softmax(dummy_x, dim = 1)\n"
      ],
      "metadata": {
        "id": "CXUYWZULLjBx",
        "outputId": "ba91a6d1-2a5d-416a-e3c0-5f52a27faeb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0900, 0.2447, 0.6652]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_x.ndim"
      ],
      "metadata": {
        "id": "b69cgpuWLyVy",
        "outputId": "9f4204bd-57b3-4913-838a-a8c23b72d16f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eflIF4-zjVTQ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}